{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music CNN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnDHitti/music_cnn/blob/main/Music_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbPn1FEjY-RR",
        "outputId": "020440ef-e375-47c8-94f7-b2b604d90e1a"
      },
      "source": [
        "# run to mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDWPmY7Lr3w5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71df6b31-be5a-4734-c635-435dfb91596f"
      },
      "source": [
        "# LOAD A TRAINING SET\n",
        "\n",
        "# change to location of the folder containing all the training data subfolders\n",
        "path=\"/content/drive/MyDrive/MusicAnalysis/Training Datasets/3 Intervals - 4kHz\" \n",
        "\n",
        "#----------------------------------------------------------------------\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # designate gpu as device\n",
        "# load in the images (turn them into tensors and also normalize them )\n",
        "# normalize takes the mean and sd of the three channels for all images - https://pytorch.org/vision/stable/transforms.html\n",
        "train = torchvision.datasets.ImageFolder(path, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.6803, 0.2475, 0.4323], [0.2560, 0.1522, 0.0951])]))\n",
        "print(train)\n",
        "# put them into trainset as tensors\n",
        "trainset = torch.utils.data.DataLoader(train,batch_size=1, shuffle=True, num_workers=0)\n",
        "# images are currently 432*228 px\n",
        "\n",
        "print(\"Loading complete... \")\n",
        "print(\"Size of trainset: \", len(trainset)) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ImageFolder\n",
            "    Number of datapoints: 12000\n",
            "    Root location: /content/drive/MyDrive/MusicAnalysis/Training Datasets/3 Intervals - 4kHz\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=[0.6803, 0.2475, 0.4323], std=[0.256, 0.1522, 0.0951])\n",
            "           )\n",
            "Loading complete... \n",
            "Size of trainset:  12000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdiQDgIc1IaL"
      },
      "source": [
        "# DEFINE THE RCNN\n",
        "\n",
        "# chance the feature amount to the amount of possible network outputs for your dataset\n",
        "featureamount=3\n",
        "\n",
        "#--------------------\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # designate gpu as device\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# network structure inspired by custom \"upchannel\" network by https://github.com/Dohppak/Music_Genre_Classification_Pytorch\n",
        "# network includes lstm layers\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 4 convolutional layers with batch normaliztion and maxpooling\n",
        "        self._convolution = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4),\n",
        "\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4)\n",
        "        )\n",
        "        # LSTM layer\n",
        "        self._RNN = nn.Sequential(\n",
        "                nn.LSTM(2048, 2048, bidirectional =True),\n",
        "                )\n",
        "        # connected layers with dropouts\n",
        "        self._connectedLayers = nn.Sequential(\n",
        "            nn.Linear(in_features=24576, out_features=1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(in_features=1024, out_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(in_features=256, out_features=featureamount))\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._convolution(x)\n",
        "        x = x.permute(3,0,1,2)\n",
        "        x = x.view(x.size(0), x.size(1), -1)\n",
        "        x, hn = self._RNN(x)\n",
        "        x = x.permute(1, 2, 0)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        out = self._connectedLayers(x)\n",
        "        return out\n",
        "\n",
        "      # defines the weights for the layers using kaming unoform method and xavier uniform method\n",
        "    def _init_weights(self, layer) -> None:\n",
        "        if isinstance(layer, nn.Conv1d):\n",
        "            nn.init.kaiming_uniform_(layer.weight)\n",
        "        elif isinstance(layer, nn.Linear):\n",
        "            nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "net = Net()\n",
        "net.to(device) # send net to gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UW3QpZS9CEu"
      },
      "source": [
        "#Declare Standard Loss Functions\n",
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()# combines logsoftmax and NLLLoss in one\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_nqkHKW9Kc5"
      },
      "source": [
        "# Declare Custom Loss Function (unfinished)\n",
        "import torch.optim as optim\n",
        "def criterion(output, target):\n",
        "    targ=target.item()\n",
        "    # pre defined tensor weights\n",
        "    w1=10\n",
        "    w2=5\n",
        "    w3=0\n",
        "    t20=torch.tensor([w1,w2,w3,w3,w3,w3,w3,w3,w3,w3])\n",
        "    t30=torch.tensor([w2,w1,w2,w3,w3,w3,w3,w3,w3,w3])\n",
        "    t40=torch.tensor([w3,w2,w1,w2,w3,w3,w3,w3,w3,w3])\n",
        "    t50=torch.tensor([w3,w3,w2,w1,w2,w3,w3,w3,w3,w3])\n",
        "    t60=torch.tensor([w3,w3,w3,w2,w1,w2,w3,w3,w3,w3])\n",
        "    t70=torch.tensor([w3,w3,w3,w3,w2,w1,w2,w3,w3,w3])\n",
        "    t80=torch.tensor([w3,w3,w3,w3,w3,w2,w1,w2,w3,w3])\n",
        "    t90=torch.tensor([w3,w3,w3,w3,w3,w3,w2,w1,w2,w3])\n",
        "    t2000=torch.tensor([w3,w3,w3,w3,w3,w3,w2,w1,w2,w3])\n",
        "    t2010=torch.tensor([w3,w3,w3,w3,w3,w3,w3,w2,w1,w2])\n",
        "    if (targ==0):\n",
        "      weight=t20.cuda()\n",
        "    if (targ==1):\n",
        "      weight=t30.cuda()\n",
        "    if (targ==2):\n",
        "      weight=t40.cuda()\n",
        "    if (targ==3):\n",
        "      weight=t50.cuda()\n",
        "    if (targ==4):\n",
        "      weight=t60.cuda()\n",
        "    if (targ==5):\n",
        "      weight=t70.cuda()\n",
        "    if (targ==6):\n",
        "      weight=t80.cuda()\n",
        "    if (targ==7):\n",
        "      weight=t90.cuda()\n",
        "    if (targ==8):\n",
        "      weight=t2000.cuda()\n",
        "    if (targ==9):\n",
        "      weight=t2010.cuda()\n",
        "    loss = torch.mean((output - target)*weight)\n",
        "    #print(loss)\n",
        "    return loss\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP48igQ7MhE8"
      },
      "source": [
        "# TRAIN NETWORK WITH TEST\n",
        "\n",
        "#change to the location of the testing data folder\n",
        "path2=\"/content/drive/MyDrive/MusicAnalysis/Testing Datasets/3 Intervals - 4kHz\" #change to where you have the year subfolders\n",
        "\n",
        "#-------------------------------------------------------\n",
        "for epoch in range(20):  # loop over the dataset x times\n",
        "    net.train()\n",
        "    epochloss=0\n",
        "    if __name__ == '__main__':\n",
        "        for i, data in enumerate(trainset, 0):\n",
        "            # seperate data and labels from tensors\n",
        "            inputs, labels = data\n",
        "\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()   \n",
        "            # zero the parameter gradients (for backwards operation [loss.backwards])\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step() # performs optimization step\n",
        "            epochloss+=loss.item()\n",
        "            #print(loss)\n",
        "\n",
        "    epochloss=epochloss/len(trainset)\n",
        "    print(\"Epoch: \",epoch+1)\n",
        "    print()\n",
        "    print(\"Average Epoch Loss: \",epochloss)\n",
        "    print()\n",
        "    print(\"testing...\")\n",
        "      # load in the images from the test set folder\n",
        "    test = torchvision.datasets.ImageFolder(path2, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.6803, 0.2475, 0.4323], [0.2560, 0.1522, 0.0951])]))\n",
        "    # put them into trainset\n",
        "    testset = torch.utils.data.DataLoader(test,batch_size=1, shuffle=True, num_workers=0)\n",
        "    testaverage=0\n",
        "    for x in range(10):\n",
        "      correct = 0\n",
        "      total = 0\n",
        "    \n",
        "      with torch.no_grad(): # Turns off the Gradient Calculations\n",
        "          for data in testset:\n",
        "              X, y = data\n",
        "              X, y = X.cuda(), y.cuda()   \n",
        "              output = net(X)\n",
        "              for idx, i in enumerate(output):\n",
        "                  if torch.argmax(i) == y[idx]: # Returns the index of the maximum value\n",
        "                      correct += 1\n",
        "                  total +=1\n",
        "      print(\"Accuracy: \", round((correct/total)*100, 3), \"%\")\n",
        "      testaverage+=round((correct/total)*100, 3)\n",
        "    testaverage=testaverage/10\n",
        "    print(\"Average: \", testaverage,\"%\")\n",
        "    print()\n",
        "    # save trained network parameters when if is true\n",
        "    if (False):\n",
        "      torch.save(net.state_dict(), '/content/drive/MyDrive/MusicAnalysis/Saved Networks for Epoch 20/savednetepoch10interval'+str(epoch)+'.pt')\n",
        "    print(\"\")\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk6jPjCmEqgi"
      },
      "source": [
        "# save trained network parameters\n",
        "torch.save(net.state_dict(), '/content/drive/MyDrive/MusicAnalysis/savednet.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqolOO3BGgoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828fa37e-c6b7-451f-c20d-c9f02d3dbcdc"
      },
      "source": [
        "# load model parameters in\n",
        "net = Net()\n",
        "net.load_state_dict(torch.load('/content/drive/MyDrive/MusicAnalysis/Saved Networks/savednetepoch10(goodOne).pt'))\n",
        "net.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu')) \n",
        "#net.eval() idk why but this usually makes it not work so i dissabled it"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (_convolution): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU()\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU()\n",
              "    (11): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
              "    (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): ReLU()\n",
              "    (15): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (_RNN): Sequential(\n",
              "    (0): LSTM(2048, 2048, bidirectional=True)\n",
              "  )\n",
              "  (_connectedLayers): Sequential(\n",
              "    (0): Linear(in_features=24576, out_features=1024, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=1024, out_features=256, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=256, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGZe9lIfWU-5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "70e555cc-0431-4462-b2b5-d834ec3b90d6"
      },
      "source": [
        "#Perform a test on the network\n",
        "\n",
        "#change this to the test folder location\n",
        "path2=\"/content/drive/MyDrive/MusicAnalysis/Testing Datasets/10 Intervals/2000s Test\" \n",
        "\n",
        "#---------------------------------------------\n",
        "for x in range(5):\n",
        "  # TEST NETWORK\n",
        "  test = torchvision.datasets.ImageFolder(path2, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.6803, 0.2475, 0.4323], [0.2560, 0.1522, 0.0951])]))\n",
        "  # put them into trainset\n",
        "  testset = torch.utils.data.DataLoader(test,batch_size=1, shuffle=True, num_workers=0)\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  net.train()\n",
        "  with torch.no_grad(): # Turns off the Gradient Calculations\n",
        "      for data in testset:\n",
        "          X, y = data\n",
        "          X, y = X.cuda(), y.cuda()   \n",
        "          output = net(X)\n",
        "          for idx, i in enumerate(output):\n",
        "              if torch.argmax(i) == y[idx]: # Returns the index of the maximum value\n",
        "                  correct += 1\n",
        "              total +=1\n",
        "  print(\"Accuracy: \", round((correct/total)*100, 3), \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ad27c91d1a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# TEST NETWORK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.6803\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2475\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4323\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.2560\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1522\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0951\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0;31m# put them into trainset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in /content/drive/MyDrive/MusicAnalysis/Testing Datasets/10 Intervals/2000s Test."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8qyohZWiYIc",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b16b49e0-4ee2-4804-b7c6-548ae4d8955f"
      },
      "source": [
        "# Load file from desktop\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-da5eee52-23cc-4b4d-a769-7adcf8911736\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-da5eee52-23cc-4b4d-a769-7adcf8911736\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Justin Bieber - Sorry.mp3 to Justin Bieber - Sorry (4).mp3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmPkTbHpoP-O"
      },
      "source": [
        "# convert to melspectrogram and test in the network \n",
        "\n",
        "#place path to file here\n",
        "path3='/content/Justin Bieber - Sorry.mp3'\n",
        "\n",
        "#------------------------\n",
        "import glob\n",
        "import librosa\n",
        "import pylab\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "      \n",
        "signalData, samplingFrequency = librosa.load(path3,sr=16000,offset=30.0, duration=30.0) # converts mp3 file to an array downsampled to 12000Hz\n",
        "librosa.util.normalize(signalData) # normalizes audio data\n",
        "        \n",
        "pylab.axis('off') # no axis\n",
        "pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[]) # Remove the white edge\n",
        "S = librosa.feature.melspectrogram(y=signalData, sr=samplingFrequency) # uses FFT to convert waveform to melspectrogram\n",
        "librosa.display.specshow(librosa.power_to_db(S, ref=np.max)) \n",
        "!mkdir dummyfolder\n",
        "!mkdir dummyfolder/dummyfolder2\n",
        "fp='/content/dummyfolder/dummyfolder2' # direct to save path\n",
        "pylab.savefig((fp+'/fileMEL'+'.png'), bbox_inches=None, pad_inches=0)      \n",
        "pylab.close()   \n",
        "\n",
        "print(\"Image Created\")\n",
        "\n",
        "\n",
        "path4=\"/content/dummyfolder\" #change to where you have the year subfolders\n",
        "test = torchvision.datasets.ImageFolder(path4, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.6803, 0.2475, 0.4323], [0.2560, 0.1522, 0.0951])]))\n",
        "# put them into trainset\n",
        "testset = torch.utils.data.DataLoader(test,batch_size=1, shuffle=True, num_workers=0)\n",
        "net.train()\n",
        "# key is asuming 3 interval network\n",
        "print()\n",
        "\n",
        "with torch.no_grad(): # Turns off the Gradient Calculations\n",
        "    for data in testset:\n",
        "        X, y = data\n",
        "        X, y = X.cuda(), y.cuda()   \n",
        "        output = net(X)\n",
        "        for idx, i in enumerate(output):\n",
        "            if (output.argmax()).item()==0:\n",
        "              print(\"This song was released some time between 1920 and 1959\")\n",
        "            if (output.argmax()).item()==1:\n",
        "              print(\"This song was released some time between 2000 and 2021\")\n",
        "            if (output.argmax()).item()==2:\n",
        "              print(\"This song was released some time between 1960 and 1999\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk7BedWx_rK5"
      },
      "source": [
        "#module for generating spectrograms from audio files\n",
        "\n",
        "import glob\n",
        "import librosa\n",
        "import eyed3\n",
        "import pylab\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# function used to create directory if it does not exitst\n",
        "def mkdir_p(mypath):\n",
        "    from errno import EEXIST\n",
        "    from os import makedirs,path\n",
        "    try:\n",
        "        makedirs(mypath)\n",
        "    except OSError as exc: \n",
        "        if exc.errno == EEXIST and path.isdir(mypath):\n",
        "            pass\n",
        "        else: raise\n",
        "\n",
        "x=0# counter for save file names\n",
        "for filepath in glob.iglob('C:/NN School Project/fma_small/**/*.mp3',recursive=True): # direct to path full of wav files\n",
        "        print(filepath)\n",
        "        \n",
        "        signalData, samplingFrequency = librosa.load(filepath,sr=12000) # converts mp3 file to an array downsampled to 12000Hz\n",
        "        librosa.util.normalize(signalData) # normalizes audio data\n",
        "        \n",
        "        \n",
        "        \n",
        "        pylab.axis('off') # no axis\n",
        "        pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[]) # Remove the white edge\n",
        "        S = librosa.feature.melspectrogram(y=signalData, sr=samplingFrequency) # uses FFT to convert waveform to melspectrogram\n",
        "        librosa.display.specshow(librosa.power_to_db(S, ref=np.max)) \n",
        "       \n",
        "        audiofile = eyed3.load(filepath)\n",
        "        year = audiofile.tag.getBestDate()\n",
        "        year = str(year)\n",
        "        year = year[ 0: 4: 1]\n",
        "        fp='C:/NN School Project/'+str(year) # direct to save path\n",
        "        if year != \"None\":\n",
        "            \n",
        "          mkdir_p(fp) # checks to see if the directory exists and if not it makes it\n",
        "          pylab.savefig((fp+'/fileMEL'+str(x)+'.png'), bbox_inches=None, pad_inches=0)\n",
        "        \n",
        "        \n",
        "        pylab.close()   \n",
        "        x=x+1\n",
        "        #plot.show()\n",
        "        print(\"Current progress: \"+ str(x) + \" out of ~8000\")\n",
        "\n",
        "print(\"Finished\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}